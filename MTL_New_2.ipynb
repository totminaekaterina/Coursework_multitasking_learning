{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6P1Cpq4oX2Ax"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet nlp==0.2.0\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import nlp\n",
    "from datasets import load_dataset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_csv_dataset(train_path, val_path, test_path, column_names, encoding=\"utf-8-sig\"):\n",
    "    train_data = pd.read_csv(train_path, encoding=encoding, sep=';')\n",
    "    val_data = pd.read_csv(val_path, encoding=encoding, sep=';')\n",
    "    test_data = pd.read_csv(test_path, encoding=encoding, sep=';')\n",
    "\n",
    "    train_data.columns = column_names\n",
    "    val_data.columns = column_names\n",
    "    test_data.columns = column_names\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "    return {\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "sts_columns = [\"sentence1\", \"sentence2\", \"similarity_score\"]\n",
    "sentiment_columns = [\"id\", \"text\", \"sentiment\"]\n",
    "paraphrase_columns = [\"sentence1\", \"sentence2\"]\n",
    "qg_columns = [\"id\", \"text\", \"predicted_question\"]\n",
    "summarization_columns = [\"id\", \"generated_text\", \"summary\"]\n",
    "title_gen_columns = [\"id\", \"text\", \"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentiment_train = \"Train_Only_Sentence_SA_train_1.csv\"\n",
    "path_to_sentiment_validation = \"Train_Only_Sentence_SA_dev_1.csv\"\n",
    "path_to_sentiment_test = \"Train_Only_Sentence_SA_test_1.csv\"\n",
    "\n",
    "path_to_sts_train = \"Train_Only_Sentence_STS_train_1.csv\"\n",
    "path_to_sts_validation = \"Train_Only_Sentence_STS_dev_1.csv\"\n",
    "path_to_sts_test = \"Train_Only_Sentence_STS_test_1.csv\"\n",
    "\n",
    "path_to_paraphrase_train = \"Train_Only_Sentence_Para_train_1.csv\"\n",
    "path_to_paraphrase_validation = \"Train_Only_Sentence_Para_dev_1.csv\"\n",
    "path_to_paraphrase_test = \"Train_Only_Sentence_Para_test_1.csv\"\n",
    "\n",
    "path_to_qg_train = \"Train_Only_Sentence_QG_train_1.csv\"\n",
    "path_to_qg_validation = \"Train_Only_Sentence_QG_dev_1.csv\"\n",
    "path_to_qg_test = \"Train_Only_Sentence_QG_test_1.csv\"\n",
    "\n",
    "path_to_summarization_train = \"Train_Only_Sentence_TextSum_train_1.csv\"\n",
    "path_to_summarization_validation = \"Train_Only_Sentence_TextSum_dev_1.csv\"\n",
    "path_to_summarization_test = \"Train_Only_Sentence_TextSum_test_1.csv\"\n",
    "\n",
    "path_to_title_gen_train = \"Train_Only_Sentence_Title_train_1.csv\"\n",
    "path_to_title_gen_validation = \"Train_Only_Sentence_Title_dev_1.csv\"\n",
    "path_to_title_gen_test = \"Train_Only_Sentence_Title_test_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"sentiment\": load_csv_dataset(\n",
    "        path_to_sentiment_train,\n",
    "        path_to_sentiment_validation,\n",
    "        path_to_sentiment_test,\n",
    "        sentiment_columns\n",
    "    ),\n",
    "    \"sts\": load_csv_dataset(\n",
    "        path_to_sts_train,\n",
    "        path_to_sts_validation,\n",
    "        path_to_sts_test,\n",
    "        sts_columns\n",
    "    ),\n",
    "    \"paraphrase\": load_csv_dataset(\n",
    "        path_to_paraphrase_train,\n",
    "        path_to_paraphrase_validation,\n",
    "        path_to_paraphrase_test,\n",
    "        paraphrase_columns\n",
    "    ),\n",
    "    \"qg\": load_csv_dataset(\n",
    "        path_to_qg_train,\n",
    "        path_to_qg_validation,\n",
    "        path_to_qg_test,\n",
    "        qg_columns\n",
    "    ),\n",
    "    \"text_summarization\": load_csv_dataset(\n",
    "        path_to_summarization_train,\n",
    "        path_to_summarization_validation,\n",
    "        path_to_summarization_test,\n",
    "        summarization_columns\n",
    "    ),\n",
    "    \"title_gen\": load_csv_dataset(\n",
    "        path_to_title_gen_train,\n",
    "        path_to_title_gen_validation,\n",
    "        path_to_title_gen_test,\n",
    "        title_gen_columns\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': {'train': Dataset({\n",
       "      features: ['id', 'text', 'sentiment'],\n",
       "      num_rows: 20\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'text', 'sentiment'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'text', 'sentiment'],\n",
       "      num_rows: 6\n",
       "  })},\n",
       " 'sts': {'train': Dataset({\n",
       "      features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "      num_rows: 19\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['sentence1', 'sentence2', 'similarity_score'],\n",
       "      num_rows: 6\n",
       "  })},\n",
       " 'paraphrase': {'train': Dataset({\n",
       "      features: ['sentence1', 'sentence2'],\n",
       "      num_rows: 20\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['sentence1', 'sentence2'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['sentence1', 'sentence2'],\n",
       "      num_rows: 6\n",
       "  })},\n",
       " 'qg': {'train': Dataset({\n",
       "      features: ['id', 'text', 'predicted_question'],\n",
       "      num_rows: 20\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'text', 'predicted_question'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'text', 'predicted_question'],\n",
       "      num_rows: 6\n",
       "  })},\n",
       " 'text_summarization': {'train': Dataset({\n",
       "      features: ['id', 'generated_text', 'summary'],\n",
       "      num_rows: 21\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'generated_text', 'summary'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'generated_text', 'summary'],\n",
       "      num_rows: 6\n",
       "  })},\n",
       " 'title_gen': {'train': Dataset({\n",
       "      features: ['id', 'text', 'generated_text'],\n",
       "      num_rows: 20\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'text', 'generated_text'],\n",
       "      num_rows: 6\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'text', 'generated_text'],\n",
       "      num_rows: 6\n",
       "  })}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "{'id': [1, 2], 'text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.'], 'sentiment': ['positive', 'positive']}\n",
      "\n",
      "sts\n",
      "{'sentence1': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.'], 'sentence2': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia скончался от передозировки наркотиками.', 'TMZ со ссылкой на источник сообщил о задержании подозреваемого в теракте в Петербурге.'], 'similarity_score': [4.36, 2.68]}\n",
      "\n",
      "paraphrase\n",
      "{'sentence1': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.'], 'sentence2': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia скончался от передозировки наркотиками.', 'TMZ со ссылкой на источник сообщил о задержании подозреваемого в теракте в Петербурге.']}\n",
      "\n",
      "qg\n",
      "{'id': [1, 2], 'text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.'], 'predicted_question': ['Были ли в интернете наркотики?', 'Полиция расследует убийства полицейских.']}\n",
      "\n",
      "text_summarization\n",
      "{'id': [1, 2], 'generated_text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет. Его жена, известная актриса и певица Лорелин Кролл, сообщила об этом на своей странице в Facebook. Она также подтвердила,', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.\\n\\nСпасатели обнаружили тело мужчины, который умер, не приходя в сознание, в четверг, 26 сентября, в поселке Харабали на северо-востоке Афганистана.\\n\\nКак рассказал источник, погибшего звали Ахмад Мохаммад аль-'], 'summary': ['В США в возрасте от 60 до 90 лет скончался один из создателей популярного видеосервиса HQ Trivia Колин Кролл.', 'В Афганистане в результате аварии погибли двое человек.']}\n",
      "\n",
      "title_gen\n",
      "{'id': [1, 2], 'text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.'], 'generated_text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет. Его жена, известная актриса и певица Лорелин Кролл, сообщила об этом на своей странице в Facebook. Она также подтвердила,', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.\\n\\nСпасатели обнаружили тело мужчины, который умер, не приходя в сознание, в четверг, 26 сентября, в поселке Харабали на северо-востоке Афганистана.\\n\\nКак рассказал источник, погибшего звали Ахмад Мохаммад аль-']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, dataset in dataset_dict.items():\n",
    "    print(task_name)\n",
    "    print(dataset_dict[task_name][\"train\"][:2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "def tokenize_for_sentiment(dataset):\n",
    "    # Токенизация для задачи анализа тональности\n",
    "    encodings = tokenizer(dataset['text'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "def tokenize_for_sts(dataset):\n",
    "    # Токенизация для задачи Semantic Text Similarity\n",
    "    encodings = tokenizer(dataset['sentence1'], dataset['sentence2'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "def tokenize_for_paraphrase(dataset):\n",
    "    # Токенизация для задачи Paraphrase Identification\n",
    "    encodings = tokenizer(dataset['sentence1'], dataset['sentence2'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "def tokenize_for_qg(dataset):\n",
    "    # Токенизация для задачи Question Generation\n",
    "    encodings = tokenizer(dataset['text'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "def tokenize_for_text_summarization(dataset):\n",
    "    # Токенизация для задачи Text Summarization\n",
    "    encodings = tokenizer(dataset['generated_text'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "def tokenize_for_title_gen(dataset):\n",
    "    # Токенизация для задачи Title Generation\n",
    "    encodings = tokenizer(dataset['text'], truncation=True, padding=True)\n",
    "    return encodings\n",
    "\n",
    "# Затем, примените соответствующую функцию токенизации к каждому поддатасету\n",
    "encoded_datasets = {}\n",
    "encoded_datasets['sentiment'] = {split: tokenize_for_sentiment(dataset) for split, dataset in dataset_dict['sentiment'].items()}\n",
    "encoded_datasets['sts'] = {split: tokenize_for_sts(dataset) for split, dataset in dataset_dict['sts'].items()}\n",
    "encoded_datasets['paraphrase'] = {split: tokenize_for_paraphrase(dataset) for split, dataset in dataset_dict['paraphrase'].items()}\n",
    "encoded_datasets['qg'] = {split: tokenize_for_qg(dataset) for split, dataset in dataset_dict['qg'].items()}\n",
    "encoded_datasets['text_summarization'] = {split: tokenize_for_text_summarization(dataset) for split, dataset in dataset_dict['text_summarization'].items()}\n",
    "encoded_datasets['title_gen'] = {split: tokenize_for_title_gen(dataset) for split, dataset in dataset_dict['title_gen'].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased', config=config)\n",
    "        \n",
    "        # Выходные слои для каждой задачи\n",
    "        self.sentiment_output = nn.Linear(config.hidden_size, 3) # Sentiment Analysis\n",
    "        self.sts_output = nn.Linear(config.hidden_size, 1) # STS\n",
    "        self.qg_output = nn.Linear(config.hidden_size, config.vocab_size) # QG\n",
    "        self.title_cond_text_output = nn.Linear(config.hidden_size, config.vocab_size) # Title conditioned text generation\n",
    "        self.summarization_output = nn.Linear(config.hidden_size, config.vocab_size) # Text summarization\n",
    "        self.paraphrase_output = nn.Linear(config.hidden_size, config.vocab_size) # Paraphrase generation\n",
    "\n",
    "    def forward(self, task_name, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        \n",
    "        if task_name == \"sentiment\":\n",
    "            output = self.sentiment_output(pooled_output)\n",
    "        elif task_name == \"sts\":\n",
    "            output = self.sts_output(pooled_output)\n",
    "        elif task_name == \"qg\":\n",
    "            output = self.qg_output(pooled_output)\n",
    "        elif task_name == \"title_gen\":\n",
    "            output = self.title_cond_text_output(pooled_output)\n",
    "        elif task_name == \"text_summarization\":\n",
    "            output = self.summarization_output(pooled_output)\n",
    "        elif task_name == \"paraphrase\":\n",
    "            output = self.paraphrase_output(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task name\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "from transformers import BertModel   \n",
    "\n",
    "config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "\n",
    "multi_task_model = MultiTaskModel(config)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_length, task):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.task = task\n",
    "        self.encoded_inputs = self.tokenize_task_data(data, task)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        encoded_input = self.encoded_inputs[self.task][idx]\n",
    "        item = {key: torch.tensor(val) for key, val in encoded_input.items()}\n",
    "        item['id'] = data['id']\n",
    "        return item\n",
    "\n",
    "    def tokenize_task_data(self, data, task):\n",
    "        if task == 'sentiment':\n",
    "            encodings = tokenize_for_sentiment(data)\n",
    "        elif task == 'sts':\n",
    "            encodings = tokenize_for_sts(data)\n",
    "        elif task == 'paraphrase':\n",
    "            encodings = tokenize_for_paraphrase(data)\n",
    "        elif task == 'qg':\n",
    "            encodings = tokenize_for_qg(data)\n",
    "        elif task == 'text_summarization':\n",
    "            encodings = tokenize_for_text_summarization(data)\n",
    "        elif task == 'title_gen':\n",
    "            encodings = tokenize_for_title_gen(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task name\")\n",
    "        return {task: encodings}\n",
    "\n",
    "\n",
    "    \n",
    "# Функция для получения DataLoader для задачи\n",
    "def get_train_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    train_data = dataset_dict[task]['train']\n",
    "    dataset = TaskDataset(train_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "def get_val_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    val_data = dataset_dict[task]['validation']\n",
    "    dataset = TaskDataset(val_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "def get_test_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    test_data = dataset_dict[task]['test']\n",
    "    dataset = TaskDataset(test_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class TaskSpecificModel(MultiTaskModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, task_name, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if task_name == \"sentiment\":\n",
    "            output = self.sentiment_output(pooled_output)\n",
    "        elif task_name == \"sts\":\n",
    "            output = self.sts_output(pooled_output)\n",
    "        elif task_name == \"qg\":\n",
    "            output = self.qg_output(pooled_output)\n",
    "        elif task_name == \"title_gen\":\n",
    "            output = self.title_cond_text_output(pooled_output)\n",
    "        elif task_name == \"text_summarization\":\n",
    "            output = self.summarization_output(pooled_output)\n",
    "        elif task_name == \"paraphrase\":\n",
    "            output = self.paraphrase_output(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task name\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'cls.predictions.decoder.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\totmi\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря меток для каждой из задач\n",
    "task_num_labels = {\n",
    "    \"sts\": 1,\n",
    "    \"sentiment\": 3,\n",
    "    \"paraphrase\": 1,\n",
    "    \"text_summarization\": 1,\n",
    "    \"title_gen\": 1,\n",
    "    \"qg\": 1\n",
    "    \n",
    "}\n",
    "\n",
    "# Создание конфигураций модели для каждой задачи\n",
    "task_configs = {\n",
    "    \"sts\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6),\n",
    "    \"sentiment\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6),\n",
    "    \"paraphrase\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6),\n",
    "    \"text_summarization\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6),\n",
    "    \"qg\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6),\n",
    "    \"title_gen\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=6, num_attention_heads=6)\n",
    "    \n",
    "}\n",
    "\n",
    "# Создание экземпляров модели для каждой задачи с задаче-специфическими гиперпараметрами\n",
    "task_models = {task: TaskSpecificModel(config, task_num_labels[task]) for task, config in task_configs.items()}\n",
    "\n",
    "# Настройка параметров оптимизатора и планировщика для каждой задачи\n",
    "task_specific_learning_rates = {\n",
    "    \"sts\": 3e-5,\n",
    "    \"sentiment\": 3e-5,\n",
    "    \"paraphrase\": 3e-5,\n",
    "    \"sentiment\": 3e-5,\n",
    "    \"text_summarization\": 3e-5,\n",
    "    \"title_gen\": 3e-5,\n",
    "    \"qg\": 3e-5\n",
    "}\n",
    "\n",
    "# Создание оптимизаторов для каждой задачи\n",
    "task_optimizers = {task: AdamW(model.parameters(), lr=lr) for task, (model, lr) in zip(task_models.keys(), zip(task_models.values(), task_specific_learning_rates.values()))}\n",
    "\n",
    "# Создание планировщиков для каждой задачи\n",
    "num_training_steps = 1000  \n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "task_schedulers = {task: get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps) for task, optimizer in task_optimizers.items()}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 3\n",
    "max_seq_length = 128\n",
    "\n",
    "train_dataloaders = {task: get_train_dataloader(task, batch_size, max_seq_length) for task in task_models.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sts': <torch.utils.data.dataloader.DataLoader at 0x1e758123940>,\n",
       " 'sentiment': <torch.utils.data.dataloader.DataLoader at 0x1e772fafb80>,\n",
       " 'paraphrase': <torch.utils.data.dataloader.DataLoader at 0x1e772fafcd0>,\n",
       " 'text_summarization': <torch.utils.data.dataloader.DataLoader at 0x1e772fafd60>,\n",
       " 'qg': <torch.utils.data.dataloader.DataLoader at 0x1e77828a910>,\n",
       " 'title_gen': <torch.utils.data.dataloader.DataLoader at 0x1e778293af0>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error\n",
    "\n",
    "# Определение функции потерь для каждой задачи\n",
    "task_criterion = {\n",
    "    \"sts\": nn.CrossEntropyLoss(),\n",
    "    \"sentiment\": nn.CrossEntropyLoss(),\n",
    "    \"qg\": nn.CrossEntropyLoss(),\n",
    "    \"paraphrase\": nn.CrossEntropyLoss(),\n",
    "    \"title_gen\": nn.CrossEntropyLoss(),\n",
    "    \"text_summarization\": nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader для каждой задачи\n",
    "batch_size = 1\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = {task: DataLoader(dataset_dict[task]['train'], batch_size=batch_size, collate_fn=data_collator) for task in task_models}\n",
    "val_dataloader = {task: DataLoader(dataset_dict[task]['validation'], batch_size=batch_size, collate_fn=data_collator) for task in task_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обучения модели\n",
    "def train(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating model for task: sts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['sentence1', 'sentence2', 'similarity_score']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m task_criterion[task]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 11\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m val_loss, val_predictions, val_labels \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader[task], criterion, device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, scheduler, criterion, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m     input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\data\\data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2902\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 2904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2905\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2906\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2907\u001b[0m     )\n\u001b[0;32m   2909\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_input:\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['sentence1', 'sentence2', 'similarity_score']"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for task in task_models:\n",
    "    print(f\"Training and evaluating model for task: {task}\")\n",
    "    model = task_models[task].to(device)\n",
    "    optimizer = task_optimizers[task]\n",
    "    scheduler = task_schedulers[task]\n",
    "    criterion = task_criterion[task]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader[task], optimizer, scheduler, criterion, device)\n",
    "    val_loss, val_predictions, val_labels = evaluate(model, val_dataloader[task], criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_models:\n",
    "    print(f\"Training and evaluating model for task: {task}\")\n",
    "    model = task_models[task].to(device)\n",
    "    optimizer = task_optimizers[task]\n",
    "    scheduler = task_schedulers[task]\n",
    "    criterion = task_criterion[task]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader[task], optimizer, scheduler, criterion, device)\n",
    "    val_loss, val_predictions, val_labels = evaluate(model, val_dataloader[task], criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление метрик качества для каждой задачи\n",
    "if task in [\"ner\", \"sentiment]:\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n",
    "    print(f\"Task: {task}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "elif task == \"sts\":\n",
    "    mse = mean_squared_error(val_labels, val_predictions)\n",
    "    print(f\"Task: {task}, MSE: {mse:.4f}\")\n",
    "else:\n",
    "    print(f\"Invalid task: {task}\")\n",
    "\n",
    "print(\"Training and evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
