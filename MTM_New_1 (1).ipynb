{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "6P1Cpq4oX2Ax"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet nlp==0.2.0\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import nlp\n",
    "from datasets import load_dataset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_csv_dataset(train_path, val_path, test_path, column_names, encoding=\"utf-8-sig\"):\n",
    "    train_data = pd.read_csv(train_path, encoding=encoding, sep=';')\n",
    "    val_data = pd.read_csv(val_path, encoding=encoding, sep=';')\n",
    "    test_data = pd.read_csv(test_path, encoding=encoding, sep=';')\n",
    "\n",
    "    train_data.columns = column_names\n",
    "    val_data.columns = column_names\n",
    "    test_data.columns = column_names\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "    return {\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "ner_columns = [\"text\", \"ner\"]\n",
    "sentiment_columns = [\"id\", \"text\", \"sentiment\", \"sentiment_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ner_train = \"Train_NER_FULL_train.csv\"\n",
    "path_to_ner_validation = \"Train_NER_FULL_dev.csv\"\n",
    "path_to_ner_test = \"Train_NER_FULL_test.csv\"\n",
    "\n",
    "path_to_sentiment_train = \"Train_Only_Sentence_SA_train_1_1.csv\"\n",
    "path_to_sentiment_validation = \"Train_Only_Sentence_SA_test_1_1.csv\"\n",
    "path_to_sentiment_test = \"Train_Only_Sentence_SA_test_1_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"ner\": load_csv_dataset(\n",
    "        path_to_ner_train,\n",
    "        path_to_ner_validation,\n",
    "        path_to_ner_test,\n",
    "        ner_columns\n",
    "    ),\n",
    "    \"sentiment\": load_csv_dataset(\n",
    "        path_to_sentiment_train,\n",
    "        path_to_sentiment_validation,\n",
    "        path_to_sentiment_test,\n",
    "        sentiment_columns\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner': {'train': Dataset({\n",
       "      features: ['text', 'ner'],\n",
       "      num_rows: 9999\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['text', 'ner'],\n",
       "      num_rows: 1000\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['text', 'ner'],\n",
       "      num_rows: 1000\n",
       "  })},\n",
       " 'sentiment': {'train': Dataset({\n",
       "      features: ['id', 'text', 'sentiment', 'sentiment_1'],\n",
       "      num_rows: 9999\n",
       "  }),\n",
       "  'validation': Dataset({\n",
       "      features: ['id', 'text', 'sentiment', 'sentiment_1'],\n",
       "      num_rows: 999\n",
       "  }),\n",
       "  'test': Dataset({\n",
       "      features: ['id', 'text', 'sentiment', 'sentiment_1'],\n",
       "      num_rows: 999\n",
       "  })}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner\n",
      "{'text': ['На', 'севере', 'граничит', 'с', 'Латвией'], 'ner': ['O', 'O', 'O', 'O', 'I-LOC']}\n",
      "\n",
      "sentiment\n",
      "{'id': [1, 2, 3, 4, 5], 'text': ['Один из создателей сервиса шестисекундных видеороликов и приложения онлайн-викторины HQ Trivia Колин Кролл умер от передозировки наркотиками в возрасте 35 лет.', 'Об этом сообщает портал TMZ со ссылкой на источник в полиции.', 'По информации издания, утром в воскресенье, 16 декабря, девушка Кролла позвонила в полицию, заявив, что не может с ним связаться.', 'После этого сотрудники правоохранительных органов приехали в квартиру в центре Манхэттена и обнаружили его тело в спальне.', 'В комнате были найдены также следы наркотиков и устройства для их употребления.'], 'sentiment': ['positive', 'positive', 'neutral', 'negative', 'neutral'], 'sentiment_1': [2, 2, 0, 1, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, dataset in dataset_dict.items():\n",
    "    print(task_name)\n",
    "    print(dataset_dict[task_name][\"train\"][:5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, config, num_ner_labels, num_sentiment_labels):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Общая часть модели для всех задач\n",
    "        self.bert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased', config=config)\n",
    "        \n",
    "        # Выходные слои для каждой задачи\n",
    "        self.ner_output = nn.Linear(config.hidden_size, num_ner_labels)\n",
    "        self.sentiment_output = nn.Linear(config.hidden_size, num_sentiment_labels)\n",
    "\n",
    "    def forward(self, task_name, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        \n",
    "        if task_name == \"ner\":\n",
    "            output = self.ner_output(pooled_output)\n",
    "        elif task_name == \"sentiment\":\n",
    "            output = self.sentiment_output(pooled_output)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task name\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "\n",
    "num_ner_labels = 9\n",
    "num_sentiment_labels = 3\n",
    "\n",
    "multi_task_model = MultiTaskModel(config, num_ner_labels, num_sentiment_labels)\n",
    "\n",
    "def get_ner_labels(tokens, entities, tokenizer, max_length):\n",
    "    labels = ['O'] * len(tokens)\n",
    "    if isinstance(entities, list):\n",
    "        for entity in entities:\n",
    "            entity_tokens = tokenizer.tokenize(entity)\n",
    "            for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                if tokens[i:i + len(entity_tokens)] == entity_tokens:\n",
    "                    labels[i] = 'B-' + entity.split('-')[0]\n",
    "                    for j in range(1, len(entity_tokens)):\n",
    "                        labels[i + j] = 'I-' + entity.split('-')[0]\n",
    "                    break\n",
    "    pad_len = max_length - len(labels)\n",
    "    if pad_len > 0:\n",
    "        labels.extend(['O'] * pad_len)\n",
    "    else:\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def encode_examples(examples, tokenizer, max_length, task):\n",
    "    encoded_inputs = {}\n",
    "    for example in examples:\n",
    "        text = example.get('text')\n",
    "        if text is None:\n",
    "            continue\n",
    "        if task == 'ner':\n",
    "            entities = example.get('ner')\n",
    "            if entities is not None:\n",
    "                encoded_input = tokenizer(text, truncation=True, padding='max_length', max_length=max_length)\n",
    "                labels = get_ner_labels(text, entities, tokenizer, max_length)\n",
    "                encoded_input['labels'] = labels\n",
    "                encoded_inputs.setdefault('ner', []).append(encoded_input)\n",
    "        else:\n",
    "            sentiment = example.get('sentiment_1')\n",
    "            if sentiment is not None:\n",
    "                encoded_input = tokenizer(text, truncation=True, padding='max_length', max_length=max_length)\n",
    "                encoded_input['labels'] = sentiment\n",
    "                encoded_inputs.setdefault('sentiment', []).append(encoded_input)\n",
    "\n",
    "    return encoded_inputs\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_length, task):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.task = task\n",
    "        self.encoded_inputs = encode_examples(data, tokenizer, max_seq_length, task)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        encoded_input = self.encoded_inputs[idx]\n",
    "        item = {key: torch.tensor(val) for key, val in encoded_input.items()}\n",
    "        item['id'] = data['id']\n",
    "        return item\n",
    "\n",
    "        encoding = self.tokenizer(text, truncation=True, max_length=self.max_seq_length, padding=\"max_length\")\n",
    "        \n",
    "        if self.task == \"ner\":\n",
    "            label_encoding = self.tokenizer(label, truncation=True, max_length=self.max_seq_length, padding=\"max_length\", is_split_into_words=True, return_offsets_mapping=True)\n",
    "            label_tensor = torch.tensor(label_encoding[\"input_ids\"], dtype=torch.long)\n",
    "        else:\n",
    "            # Здесь обрабатываются метки для других задач\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        # Создание словаря с ключами input_ids, attention_mask и labels\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(encoding[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": label_tensor,\n",
    "        }\n",
    "    \n",
    "# Функция для получения DataLoader для задачи\n",
    "def get_train_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    train_data = dataset_dict[task]['train']\n",
    "    dataset = TaskDataset(train_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "def get_val_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    val_data = dataset_dict[task]['validation']\n",
    "    dataset = TaskDataset(val_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "def get_test_dataloader(task, batch_size=1, max_seq_length=128):\n",
    "    test_data = dataset_dict[task]['test']\n",
    "    dataset = TaskDataset(test_data, tokenizer, max_seq_length, task)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return dataloader\n",
    "\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class TaskSpecificModel(MultiTaskModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config, num_labels, num_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'cls.predictions.decoder.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря меток для каждой из задач\n",
    "task_num_labels = {\n",
    "    \"ner\": 9,\n",
    "    \"sentiment\": 3\n",
    "}\n",
    "\n",
    "# Создание конфигураций модели для каждой задачи\n",
    "task_configs = {\n",
    "    \"ner\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=8, num_attention_heads=8),\n",
    "    \"sentiment\": BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', num_hidden_layers=8, num_attention_heads=8)\n",
    "}\n",
    "\n",
    "# Создание экземпляров модели для каждой задачи с задаче-специфическими гиперпараметрами\n",
    "task_models = {task: TaskSpecificModel(config, task_num_labels[task]) for task, config in task_configs.items()}\n",
    "\n",
    "# Настройка параметров оптимизатора и планировщика для каждой задачи\n",
    "task_specific_learning_rates = {\n",
    "    \"ner\": 3e-5,\n",
    "    \"sentiment\": 3e-5\n",
    "}\n",
    "\n",
    "# Создание оптимизаторов для каждой задачи\n",
    "task_optimizers = {task: AdamW(model.parameters(), lr=lr) for task, (model, lr) in zip(task_models.keys(), zip(task_models.values(), task_specific_learning_rates.values()))}\n",
    "\n",
    "# Создание планировщиков для каждой задачи\n",
    "num_training_steps = 1000  \n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "task_schedulers = {task: get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps) for task, optimizer in task_optimizers.items()}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 3\n",
    "max_seq_length = 128\n",
    "\n",
    "train_dataloaders = {task: get_train_dataloader(task, batch_size, max_seq_length) for task in task_models.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner': <torch.utils.data.dataloader.DataLoader at 0x207fca2fa90>,\n",
       " 'sentiment': <torch.utils.data.dataloader.DataLoader at 0x20804865fa0>}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error\n",
    "\n",
    "# Определение функции потерь для каждой задачи\n",
    "task_criterion = {\n",
    "    \"ner\": nn.CrossEntropyLoss(),\n",
    "    \"sentiment\": nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader для каждой задачи\n",
    "batch_size = 1\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = {task: DataLoader(dataset_dict[task]['train'], batch_size=batch_size, collate_fn=data_collator) for task in task_models}\n",
    "val_dataloader = {task: DataLoader(dataset_dict[task]['validation'], batch_size=batch_size, collate_fn=data_collator) for task in task_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обучения модели\n",
    "def train(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        item, keys = batch\n",
    "        input_ids, attention_mask, labels = item[keys[0]].to(device), item[keys[1]].to(device), item[keys[2]].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating model for task: ner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text', 'ner']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m task_criterion[task]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 11\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m val_loss, val_predictions, val_labels \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader[task], criterion, device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[172], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, scheduler, criterion, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m     item, keys \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\data\\data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2902\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 2904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2905\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2906\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2907\u001b[0m     )\n\u001b[0;32m   2909\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_input:\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text', 'ner']"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for task in task_models:\n",
    "    print(f\"Training and evaluating model for task: {task}\")\n",
    "    model = task_models[task].to(device)\n",
    "    optimizer = task_optimizers[task]\n",
    "    scheduler = task_schedulers[task]\n",
    "    criterion = task_criterion[task]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader[task], optimizer, scheduler, criterion, device)\n",
    "    val_loss, val_predictions, val_labels = evaluate(model, val_dataloader[task], criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_models:\n",
    "    print(f\"Training and evaluating model for task: {task}\")\n",
    "    model = task_models[task].to(device)\n",
    "    optimizer = task_optimizers[task]\n",
    "    scheduler = task_schedulers[task]\n",
    "    criterion = task_criterion[task]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_dataloader[task], optimizer, scheduler, criterion, device)\n",
    "    val_loss, val_predictions, val_labels = evaluate(model, val_dataloader[task], criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление метрик качества для каждой задачи\n",
    "if task in [\"ner\", \"sentiment]:\n",
    "    accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n",
    "    print(f\"Task: {task}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "elif task == \"sts\":\n",
    "    mse = mean_squared_error(val_labels, val_predictions)\n",
    "    print(f\"Task: {task}, MSE: {mse:.4f}\")\n",
    "else:\n",
    "    print(f\"Invalid task: {task}\")\n",
    "\n",
    "print(\"Training and evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
