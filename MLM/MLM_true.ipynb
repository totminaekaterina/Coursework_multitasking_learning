{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "data = pd.read_csv('Train_Only_Sentence.csv', encoding='utf-8-sig', sep=';')\n",
    "data.head()\n",
    "\n",
    "\n",
    "data.fillna(\"\", inplace=True)\n",
    "# Инициализация токенизатора и модели\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Кодирование текстовых данных в токены и перевод на устройство\n",
    "inputs = tokenizer(data['text'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Получение маскированных токенов\n",
    "masked_inputs = inputs['input_ids'].clone().detach().cpu().numpy()\n",
    "mask = np.logical_and(np.random.rand(*masked_inputs.shape) < 0.15, inputs['attention_mask'].clone().detach().cpu().numpy())\n",
    "masked_inputs = np.where(mask, -100, masked_inputs)\n",
    "masked_inputs = torch.tensor(masked_inputs).to(device)\n",
    "\n",
    "# Обучение модели с оптимизаторами l1 и l2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01, eps=1e-06)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    progress_bar = tqdm(range(0, len(data), batch_size))\n",
    "    for i in progress_bar:\n",
    "        batch_inputs = {key: val[i:i+batch_size] for key, val in inputs.items()}\n",
    "        batch_masked_inputs = masked_inputs[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch_inputs['input_ids'], attention_mask=batch_inputs['attention_mask'], labels=batch_masked_inputs)\n",
    "        loss = outputs[0]\n",
    "        running_loss += loss.mean().item() * batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        # L1 regularization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l1_regularization = torch.norm(param, 1)\n",
    "                loss += 0.0001 * l1_regularization.to(device)\n",
    "        \n",
    "        # L2 regularization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l2_regularization = torch.norm(param, 2)\n",
    "                loss += 0.0001 * l2_regularization.to(device)\n",
    "                \n",
    "        optimizer.step()\n",
    "\n",
    "        # Предсказание и сохранение значений для метрик\n",
    "        y_true.extend(batch_masked_inputs.cpu().numpy().flatten())\n",
    "        y_pred.extend(outputs[1].argmax(dim=-1).cpu().numpy().flatten())\n",
    "        progress_bar.set_description(f\"Epoch: {epoch + 1}/{epochs}, Loss: {running_loss / (i + batch_size):.5f}\")\n",
    "        \n",
    "        # Рассчитываем метрики на обучающем наборе данных\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Training F1-score: {f1:.5f}, Training Accuracy: {acc:.5f}\")\n",
    "\n",
    "        # Пересчет оптимизатора\n",
    "        scheduler.step(running_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
